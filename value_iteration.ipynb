{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sys,os\n",
    "import torch\n",
    "import random\n",
    "curr_path = os.path.abspath('')\n",
    "parent_path = os.path.dirname(curr_path)\n",
    "sys.path.append(parent_path)\n",
    "from rl.envs.simple_grid import DrunkenWalkEnv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Set Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## fix the random seed\n",
    "def all_seed(env,seed = 1):\n",
    "    env.seed(seed) \n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = DrunkenWalkEnv(map_name=\"theAlley\")\n",
    "all_seed(env, seed = 1)  ## random seed is 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Value Iteration Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def value_iteration(env, theta=0.005, discount_factor=0.7):\n",
    "    Q = np.zeros((env.nS, env.nA)) # initialize value table Q\n",
    "    count = 0\n",
    "    while True:\n",
    "        delta = 0.0\n",
    "        Q_tmp = np.zeros((env.nS, env.nA))\n",
    "        for state in range(env.nS):\n",
    "            for a in range(env.nA):\n",
    "                accum = 0.0\n",
    "                reward_total = 0.0\n",
    "                for prob, next_state, reward, done in env.P[state][a]:\n",
    "                    accum += prob* np.max(Q[next_state, :])\n",
    "                    reward_total += prob * reward # reward is also an expection, which indicates how much reward you will receive when you arrive at some state.\n",
    "                Q_tmp[state, a] = reward_total + discount_factor * accum\n",
    "                delta = max(delta, abs(Q_tmp[state, a] - Q[state, a]))\n",
    "        Q = Q_tmp\n",
    "        count += 1\n",
    "        if delta < theta or count > 1000:\n",
    "            break \n",
    "    return Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13 4\n"
     ]
    }
   ],
   "source": [
    "print(env.nS,env.nA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(13, 4)\n",
      "[[1.39599386e+143 1.57049310e+143 2.79198773e+143 1.57049310e+143]\n",
      " [1.74499233e+143 3.35911024e+143 6.28197239e+143 3.35911024e+143]\n",
      " [3.92623274e+143 7.55799803e+143 1.41344379e+144 7.55799803e+143]\n",
      " [8.83402367e+143 1.70054956e+144 3.18024852e+144 1.70054956e+144]\n",
      " [1.98765533e+144 3.57777959e+144 7.15555918e+144 3.57777959e+144]\n",
      " [4.47222449e+144 8.60903213e+144 1.61000081e+145 8.60903213e+144]\n",
      " [1.00625051e+145 1.93703223e+145 3.62250183e+145 1.93703223e+145]\n",
      " [2.26406365e+145 4.35832252e+145 8.15062912e+145 4.35832252e+145]\n",
      " [5.09414320e+145 9.16945777e+145 1.83389155e+146 9.16945777e+145]\n",
      " [1.14618222e+146 2.20640077e+146 4.12625599e+146 2.20640077e+146]\n",
      " [2.57891000e+146 4.96440174e+146 9.28407599e+146 4.96440174e+146]\n",
      " [5.80254749e+146 1.11699039e+147 2.08891710e+147 1.11699039e+147]\n",
      " [3.65560492e+147 4.56950615e+147 4.70006347e+147 4.56950615e+147]]\n"
     ]
    }
   ],
   "source": [
    "Q = value_iteration(env)\n",
    "print(Q.shape)\n",
    "print(Q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n"
     ]
    }
   ],
   "source": [
    "policy = np.zeros([env.nS, env.nA]) \n",
    "for state in range(env.nS):\n",
    "    best_action = np.argmax(Q[state, :]) \n",
    "    policy[state, best_action] = 1\n",
    "\n",
    "policy = [int(np.argwhere(policy[i]==1)) for i in range(env.nS) ]\n",
    "print(policy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_episode = 2000 \n",
    "def test(env,policy):\n",
    "    \n",
    "    rewards = []  \n",
    "    success = []  \n",
    "    for i_ep in range(num_episode):\n",
    "        ep_reward = 0  \n",
    "        state = env.reset() \n",
    "        while True:\n",
    "            action = policy[state]\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            state = next_state\n",
    "            ep_reward += reward\n",
    "            if done:\n",
    "                break\n",
    "        if state==12: \n",
    "            success.append(1)\n",
    "        else:\n",
    "            success.append(0)\n",
    "        rewards.append(ep_reward)\n",
    "    acc_suc = np.array(success).sum()/num_episode\n",
    "    print(\"accuracy is\", acc_suc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy is 0.6575\n"
     ]
    }
   ],
   "source": [
    "test(env, policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:rl_jl]",
   "language": "python",
   "name": "conda-env-rl_jl-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
